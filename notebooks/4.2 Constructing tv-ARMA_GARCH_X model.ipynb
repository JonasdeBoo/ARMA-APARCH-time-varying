{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Constructing AR-GARCH-X model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This template is used to calculate and show plots of the AR-GARCH-X model, which makes use of the ArGarchX class. This class has several methods in order to compute, with the help of the Maximum Likelihood method the estimators that maximize the Quasi Log Likelihood. Via this template several models will be constructed and tested, in order to check the effect of public sentiment on volatility, and it will be checked whether adding these variables increases predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Load packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, load the packages, data and colors for the main analysis and for the construction of plots, also define export locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "import os, sys\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load data that returns tweets\n",
    "sys.path.insert(0, os.path.abspath(r'C:\\Users\\Jonas\\PycharmProjects\\TwitterSentimentGARCH2021\\Code\\GARCH model\\tvGARCH models'))\n",
    "from tv_garch_models import tvArmaApARCHX, tvArmaXapARCH, tvArmaApArchXGarch\n",
    "from tv_garch_estimation import QuasiMaximumLikelihoodEstimator\n",
    "\n",
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Colors for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['seagreen', 'mediumaquamarine', 'steelblue', 'cornflowerblue', 'navy', 'black']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load company name data and DataFrame per company containing all the sentiment, return and control variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of data + file name and location of storage\n",
    "company_loc = r'C:\\Users\\Jonas\\Documents\\Data'\n",
    "file_name_comp = '\\company_ticker_list_all.xlsx'\n",
    "\n",
    "# Access company names DataFrame\n",
    "df_comp_names = pd.read_excel(company_loc + file_name_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify location where all company specific data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify name and location\n",
    "data_loc = r'C:\\Users\\Jonas\\Documents\\Data\\Total_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify location where to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location where daily sentiment scores must be stored\n",
    "store_loc = r'C:\\Users\\Jonas\\Documents\\Data\\Results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs the main analysis and will calculate the results for each company in the selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Create models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the different columns in the total dataset that need to be evaluated and serve as input into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible exogenous columns\n",
    "control_cols = ['VIX', 'TEDRATE']\n",
    "sent_cols = ['sentiment', 'n_tweets', 'n_interactions']\n",
    "\n",
    "# Define data on which to impose GARCH structure, and provide column names\n",
    "x_garch_cols = [f'sigma2_{col}' for col in sent_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Calculate optimal number of lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a grid search for various values of h, in the second model, where the sentiment variables enter the model in the conditional volatility equation. The value for h that maximizes the QLL is used throughout the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:\n",
    "    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    \n",
    "    # Calculate optimal parameters for each model\n",
    "    h_vals = [10, 15, 25, 50, 100]\n",
    "    dict_h = {}\n",
    "    \n",
    "    for h in h_vals:   \n",
    "        qmle = QuasiMaximumLikelihoodEstimator(df_total, 'returns', control_cols, h, lags_arma,\n",
    "                                               sent_cols, list(lags_per_model[2]), params=None, \n",
    "                                               model_type='asym')\n",
    "    \n",
    "        minimization_result, psi_hat, likelihood, df_params = qmle.optimize_likelihood()\n",
    "        \n",
    "        dict_h[h] = likelihood\n",
    "    \n",
    "        print(f\"Likelihood with the smooth transition operator h = {h} is {likelihood}\")\n",
    "        \n",
    "    print(f\"Among the suggested values for h, the best value is {max(dict_h, key=dict_h.get)}, with likelihood {max(dict_h)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Calculate optimal time-varying parameters and standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, for each model, the optimal parameters are calculated and the standard error of these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:\n",
    "    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    \n",
    "    # Calculate optimal parameters for each model\n",
    "    \n",
    "    # - Model ARMA-X-apARCH\n",
    "    qmle1 = QuasiMaximumLikelihoodEstimator(df_total, 'returns', control_cols, h, lags_arma,\n",
    "                                            sent_cols, list(lags_per_model[1]), params=None, \n",
    "                                            model_type='mean-x')\n",
    "    \n",
    "    minimization_result1, psi_hat1, likelihood1, df_params1 = qmle1.optimize_likelihood()\n",
    "    \n",
    "    vcov1 = qmle1.vcov(psi_hat1)\n",
    "    df_params1['errors'] = np.sqrt(np.diag(vcov1))\n",
    "    df_params1['t-stat'] = psi_hat1 / np.sqrt(np.diag(vcov1))\n",
    "    \n",
    "    # - Model ARMA-apARCH-X\n",
    "    qmle2 = QuasiMaximumLikelihoodEstimator(df_total, 'returns', control_cols, h, lags_arma,\n",
    "                                            sent_cols, list(lags_per_model[2]), params=None, \n",
    "                                            model_type='asym')\n",
    "    \n",
    "    minimization_result2, psi_hat2, likelihood2, df_params2 = qmle2.optimize_likelihood()\n",
    "    \n",
    "    vcov2 = qmle2.vcov(psi_hat2)\n",
    "    df_params2['errors'] = np.sqrt(np.diag(vcov2))\n",
    "    df_params2['t-stat'] = psi_hat2  /  np.sqrt(np.diag(vcov2))\n",
    "    \n",
    "    # - Model ARMA-apARCH-(X-ARMA-GARCH)\n",
    "    exog_lags = list(lags_per_model[3])[:len(sent_cols)]\n",
    "    x_garch_lags = list(lags_per_model[3])[len(sent_cols):]\n",
    "    qmle3 = QuasiMaximumLikelihoodEstimator(df_total, 'returns', control_cols, h, lags_arma,\n",
    "                                            sent_cols, exog_lags, params=None,\n",
    "                                            model_type='x-garch', x_garch_cols=x_garch_cols, \n",
    "                                            x_garch_lags=x_garch_lags)\n",
    "    \n",
    "    minimization_result3, psi_hat3, likelihood3, df_params3 = qmle3.optimize_likelihood()\n",
    "    \n",
    "    vcov3 = qmle3.vcov(psi_hat3)\n",
    "    df_params3['errors'] = np.sqrt(np.diag(vcov3))\n",
    "    df_params3['t-stat'] = psi_hat3  / np.sqrt(np.diag(vcov3))\n",
    "    \n",
    "    # Store all parameters into a .csv file\n",
    "    df_params1.to_csv(data_loc + f'\\\\parameters\\\\model1\\\\tv params {company}.csv', index=False)\n",
    "    df_params2.to_csv(data_loc + f'\\\\parameters\\\\model2\\\\tv params {company}.csv', index=False)\n",
    "    df_params3.to_csv(data_loc + f'\\\\parameters\\\\model3\\\\tv params {company}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Likelihood calculation functionality that can be used to quickly calculate the likelihood given residuals and conditional volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_log_likelihood(sigma2, et):\n",
    "    # QMLE from Franq and Thieu\n",
    "    lls = np.log(sigma2) + ((et ** 2) / sigma2) + np.log(2 * np.pi)\n",
    "\n",
    "    # Calculate Quasi Maximum Likelihood\n",
    "    ll = np.nan_to_num(lls).sum()\n",
    "\n",
    "    return -ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, calculate and plot the constructed values of the conditional volatility model, conditional mean model and the distribution of the innovations $z_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\tv params {company}.csv')\n",
    "    df_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\tv params {company}.csv')\n",
    "    df_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\tv params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model     \n",
    "    model1 = tvArmaXapARCH(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[1]), \n",
    "                         params = df_params1['psi_hat'].tolist())\n",
    "    \n",
    "    model2 = tvArmaApARCHX(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[2]), \n",
    "                         params = df_params2['psi_hat'].tolist())\n",
    "    \n",
    "    exog_lags = list(lags_per_model[3])[:len(sent_cols)]\n",
    "    x_garch_lags = list(lags_per_model[3])[len(sent_cols):]\n",
    "    \n",
    "    model3 = tvArmaApArchXGarch(df_total, 'returns', control_cols, h, lags_arma, sent_cols, exog_lags, \n",
    "                              params = df_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "\n",
    "    sigma2_1, et_1 = model1.conditional_volatility()\n",
    "    sigma2_2, et_2 = model2.conditional_volatility()\n",
    "    sigma2_3, et_3 = model3.conditional_volatility()\n",
    "    \n",
    "    vars_, ets = [sigma2_1, sigma2_2, sigma2_3], [et_1, et_2, et_3]\n",
    "    \n",
    "    # Calculate likelihood for all models\n",
    "    ll_1, ll_2, ll_3 = quasi_log_likelihood(sigma2_1, et_1), quasi_log_likelihood(sigma2_2, et_2), quasi_log_likelihood(sigma2_3, et_3)\n",
    "    likelihoods = [ll_1, ll_2, ll_3]\n",
    "    \n",
    "    # Now, construct plots for all models\n",
    "    fig, axs = plt.subplots(figsize = (20,4), nrows = 1, ncols = 3)\n",
    "    \n",
    "    first_date, last_date = df_total.date.iloc[0], df_total.date.iloc[-1]\n",
    "    n = 150  # keeps every 150th label (around half a year)\n",
    "\n",
    "    for j in range(len(axs)):\n",
    "        axs[j].plot(np.sqrt(ets[j] ** 2), c='black', linestyle='-.', alpha=0.7)\n",
    "        axs[j].plot(np.sqrt(sigma2), c='yellow', linestyle='--', alpha=0.85)\n",
    "        axs[j].plot(np.sqrt(vars_[j]), c=colors[j], alpha=0.7)\n",
    "        \n",
    "        # Set title and xticklabels\n",
    "        axs[j].set_title(f'Conditional volatility of: {company}' + '\\n' f'QLL {likelihoods[j]}' + '\\n' + f'Period: {first_date} to {last_date}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Plot KDE plot of innovations\n",
    "    z_ts = [et_1 /np.sqrt(sigma2_1), et_2 / np.sqrt(sigma2_2), et_3 / np.sqrt(sigma2_3)]\n",
    "\n",
    "    fig2, axs = plt.subplots(figsize = (20,4), nrows = 1, ncols = len(z_ts))\n",
    "    for i, ax in enumerate(axs):\n",
    "        sns.kdeplot(z_ts[i], ax=ax, color=colors[i])\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.hist(z_ts[i], color=colors[i+2], bins=25, rwidth=0.9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Store figures as PNG\n",
    "    fig.savefig(store_loc + f'\\\\plots\\\\tv plot conditional vol {company}')\n",
    "    fig2.savefig(store_loc + f'\\\\plots\\\\tv kde plot of innovations of {company}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Export parameter DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates and exports the DataFrames with parameter estimates of each model included in this research for all companies in this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create empty DataFrames to store results in\n",
    "df_psi_hat_1 = pd.DataFrame()\n",
    "df_psi_hat_2 = pd.DataFrame()\n",
    "df_psi_hat_3 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df_comp_names['Company'])):\n",
    "    company = df_comp_names['Company'].iloc[i]\n",
    "\n",
    "    # Store all parameters into a .csv file\n",
    "    df_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\tv params {company}.csv')\n",
    "    df_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\tv params {company}.csv')\n",
    "    df_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\tv params {company}.csv')\n",
    "    \n",
    "    # Compile errors in parentheses\n",
    "    errors1 = df_params1['errors'].apply(lambda x: '%.1e' % x).apply(lambda x: f' ({x})')\n",
    "    \n",
    "    # Get p-value based on t-statistic\n",
    "    p = df_params1['t-stat'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)]))    \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params1)):\n",
    "        df_params.loc[len(df_params)] = [df_params1.iloc[j]['param names'], \n",
    "                                         df_params1.iloc[j]['psi_hat'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = ['error_' + df_params1.iloc[j]['param names'], errors1[j]]\n",
    "            \n",
    "    df_params1 = df_params\n",
    "    \n",
    "    # Compile errors in parentheses\n",
    "    errors2 = df_params2['errors'].apply(lambda x: '%.1e' % x).apply(lambda x: f' ({x})')\n",
    "    \n",
    "    # Get p-value based on t-statistic\n",
    "    p = df_params2['t-stat'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)]))\n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params2)):\n",
    "        df_params.loc[len(df_params)] = [df_params2.iloc[j]['param names'], \n",
    "                                         df_params2.iloc[j]['psi_hat'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = ['error_' + df_params2.iloc[j]['param names'], errors2[j]]\n",
    "            \n",
    "    df_params2 = df_params\n",
    "   \n",
    "    # Compile errors in parentheses\n",
    "    errors3 = df_params3['errors'].apply(lambda x: '%.1e' % x).apply(lambda x:  f' ({x})')\n",
    "    \n",
    "    # Get p-value based on t-statistic\n",
    "    p = df_params3['t-stat'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)])) \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params3)):\n",
    "        df_params.loc[len(df_params)] = [df_params3.iloc[j]['param names'], \n",
    "                                         df_params3.iloc[j]['psi_hat'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = ['error_' + df_params3.iloc[j]['param names'], errors3[j]]\n",
    "            \n",
    "    df_params3 = df_params\n",
    "    \n",
    "    if i == 0:        \n",
    "        df_psi_hat_1 = df_params1\n",
    "        df_psi_hat_2 = df_params2\n",
    "        df_psi_hat_3 = df_params3\n",
    "    else:\n",
    "        df_psi_hat_1 = df_psi_hat_1.merge(df_params1[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer') \n",
    "\n",
    "        df_psi_hat_2 = df_psi_hat_2.merge(df_params2[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "    \n",
    "\n",
    "        df_psi_hat_3 = df_psi_hat_3.merge(df_params3[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "        \n",
    "    # Save DataFrames to .csv\n",
    "    df_psi_hat_1.to_csv(data_loc + f'\\\\parameters\\\\tv params model1.csv')\n",
    "    df_psi_hat_2.to_csv(data_loc + f'\\\\parameters\\\\tv params model2.csv')\n",
    "    df_psi_hat_3.to_csv(data_loc + f'\\\\parameters\\\\tv params model3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
