{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Constructing AR-GARCH-X model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This template is used to calculate and show plots of the AR-GARCH-X model, which makes use of the ArGarchX class. This class has several methods in order to compute, with the help of the Maximum Likelihood method the estimators that maximize the Quasi Log Likelihood. Via this template several models will be constructed and tested, in order to check the effect of public sentiment on volatility, and it will be checked whether adding these variables increases predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Load packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, load the packages, data and colors for the main analysis and for the construction of plots, also define export locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "import os, sys\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from scipy.stats import chi2\n",
    "import scipy.optimize as opt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load data that returns tweets\n",
    "sys.path.insert(0, os.path.abspath(r'C:\\Users\\Jonas\\PycharmProjects\\TwitterSentimentGARCH2021\\Code\\GARCH model'))\n",
    "from garch_models import ArmaApARCHX, ArmaXapARCH, ArmaApArchXGarch, ExogGarch\n",
    "from mean_model import MeanModel\n",
    "from qml_estimation import QuasiMaximumLikelihoodEstimator\n",
    "from bic_find_lags import LagsSelection\n",
    "\n",
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Colors for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['seagreen', 'mediumaquamarine', 'steelblue', 'cornflowerblue', 'navy', 'black']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load company name data and DataFrame per company containing all the sentiment, return and control variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of data + file name and location of storage\n",
    "company_loc = r'C:\\Users\\Jonas\\Documents\\Data'\n",
    "file_name_comp = '\\company_ticker_list_all.xlsx'\n",
    "\n",
    "# Access company names DataFrame\n",
    "df_comp_names = pd.read_excel(company_loc + file_name_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify location where all company specific data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify name and location\n",
    "data_loc = r'C:\\Users\\Jonas\\Documents\\Data\\Total_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify location where to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location where daily sentiment scores must be stored\n",
    "store_loc = r'C:\\Users\\Jonas\\Documents\\Data\\Results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs the main analysis and will calculate the results for each company in the selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Create models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the different columns in the total dataset that need to be evaluated and serve as input into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible exogenous columns\n",
    "control_cols = ['VIX', 'TEDRATE']\n",
    "sent_cols = ['sentiment', 'n_tweets', 'n_interactions']\n",
    "\n",
    "exog_cols = sent_cols\n",
    "\n",
    "# Define data on which to impose GARCH structure, and provide column names\n",
    "x_garch_cols = [f'sigma2_{col}' for col in sent_cols]\n",
    "\n",
    "# Define possible model types\n",
    "model_types = ['mean-x', 'asym', 'x-garch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Calculate exogenous GARCH processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the parameters for the exogenous GARCH processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:   \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    df_xgarch_params = pd.DataFrame()\n",
    "\n",
    "    for j, col in enumerate(sent_cols):\n",
    "        qmle = QuasiMaximumLikelihoodEstimator(df_total, [col], model_type='garch')\n",
    "\n",
    "        # Optimize Exogenous GARCH process\n",
    "        minimization_result, psi_hat, likelihood, df_params = qmle.optimize_likelihood()\n",
    "        \n",
    "        # Calculate exogenous GARCH process\n",
    "        sigma2_x, et_x = ExogGarch(df_total, [col], psi_hat).conditional_volatility()\n",
    "        df_total[f'sigma2_{col}'] = sigma2_x\n",
    "        df_total[f'et_{col}'] = et_x\n",
    "\n",
    "        # Calculate Variance-Covariance matrix\n",
    "        vcov = qmle.vcov(psi_hat)\n",
    "        df_params[f'errors'] = np.sqrt(np.diag(vcov))\n",
    "        df_params[f't-stat'] = psi_hat / np.sqrt(np.diag(vcov))\n",
    "\n",
    "        param_names = df_params['param names']\n",
    "        params = df_params.psi_hat\n",
    "\n",
    "        # Add parameter estimates to df_xgarch_params\n",
    "        df_xgarch_params = pd.concat([df_xgarch_params, df_params], axis=1)\n",
    "       \n",
    "    # Store variables of sentiment regression    \n",
    "    df_xgarch_params.to_csv(data_loc + f'\\\\exogenous garch data\\\\xgarch_params {company}.csv')\n",
    "    df_total.to_csv(data_loc + data_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Find lags of mean and GARCH model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the lags of the ARMA(P,Q) conditional mean model and the GARCH-X(L_k) conditional volatility model are found. This is done with help of the `LagsSelection` class, which conditional finds the ARMA lags that minimize the BIC, and can both dependent and independent find the optimal lags of the conditional volatility specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create index\n",
    "index = ['ARMA lags'] + [f'X-lags for model: {model}' for model in model_types]\n",
    "\n",
    "# Instantiate DataFrame to store results\n",
    "df_model_specification = pd.DataFrame(index=index)\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:   \n",
    "    # Create several different models\n",
    "    lags_per_model = []\n",
    "    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "\n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Calculate ARMA lags disregarding how the exogenous variables enter the model\n",
    "    lags_arma, bic_arma = LagsSelection(df=df_total, returns_col='returns').find_arma_lags()\n",
    "    lags_per_model += [tuple(lags_arma)]\n",
    "    \n",
    "    # Create several different models\n",
    "    for model in model_types:\n",
    "        lags_vol = LagsSelection(df=df_total, returns_col='returns', arma_lags=lags_arma, exog_cols=exog_cols,\n",
    "                                             model_type=model, x_garch_cols=x_garch_cols).find_exog_lags()\n",
    "        lags_per_model += [tuple(lags_vol)]       \n",
    "            \n",
    "    df_model_specification[company] = lags_per_model\n",
    "    \n",
    "# Store DataFrame with model characterstics as .csv file\n",
    "df_model_specification.to_csv(data_loc + f'\\\\lags\\\\lags_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Calculate optimal parameters and standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, for each model, the optimal parameters are calculated and the standard error of these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:\n",
    "    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    \n",
    "    # Calculate optimal parameters for each model\n",
    "    \n",
    "    # - Benchmark model\n",
    "    qmle = QuasiMaximumLikelihoodEstimator(df_total, 'returns', lags_arma, model_type='asym', exog_cols=None, lags_exog=None)\n",
    "    \n",
    "    minimization_result, psi_hat, likelihood, df_params = qmle.optimize_likelihood()\n",
    "    \n",
    "    vcov = qmle.vcov(psi_hat)\n",
    "    df_params['errors'] = np.sqrt(np.diag(vcov))\n",
    "    df_params['t-stat'] = psi_hat / np.sqrt(np.diag(vcov))\n",
    "    \n",
    "    \n",
    "    # - Model ARMA-X-apARCH\n",
    "    qmle1 = QuasiMaximumLikelihoodEstimator(df_total, 'returns', lags_arma,\n",
    "                                            exog_cols, list(lags_per_model[1]), params=None, \n",
    "                                            model_type='mean-x')\n",
    "    \n",
    "    minimization_result1, psi_hat1, likelihood1, df_params1 = qmle1.optimize_likelihood()\n",
    "    \n",
    "    vcov1 = qmle1.vcov(psi_hat1)\n",
    "    df_params1['errors'] = np.sqrt(np.diag(vcov1))\n",
    "    df_params1['t-stat'] = psi_hat1 / np.sqrt(np.diag(vcov1))\n",
    "    \n",
    "    # - Model ARMA-apARCH-X\n",
    "    qmle2 = QuasiMaximumLikelihoodEstimator(df_total, 'returns', lags_arma,\n",
    "                                            exog_cols, list(lags_per_model[2]), params=None, \n",
    "                                            model_type='asym')\n",
    "    \n",
    "    minimization_result2, psi_hat2, likelihood2, df_params2 = qmle2.optimize_likelihood()\n",
    "    \n",
    "    vcov2 = qmle2.vcov(psi_hat2)\n",
    "    df_params2['errors'] = np.sqrt(np.diag(vcov2))\n",
    "    df_params2['t-stat'] = psi_hat2  /  np.sqrt(np.diag(vcov2))\n",
    "    \n",
    "    # - Model ARMA-apARCH-(X-ARMA-GARCH)\n",
    "    exog_lags = list(lags_per_model[3])[:len(exog_cols)]\n",
    "    x_garch_lags = list(lags_per_model[3])[len(exog_cols):]\n",
    "    qmle3 = QuasiMaximumLikelihoodEstimator(df_total, 'returns', lags_arma,\n",
    "                                            exog_cols, exog_lags, params=None,\n",
    "                                            model_type='x-garch', x_garch_cols=x_garch_cols, x_garch_lags=x_garch_lags)\n",
    "    \n",
    "    minimization_result3, psi_hat3, likelihood3, df_params3 = qmle3.optimize_likelihood()\n",
    "    \n",
    "    vcov3 = qmle3.vcov(psi_hat3)\n",
    "    df_params3['errors'] = np.sqrt(np.diag(vcov3))\n",
    "    df_params3['t-stat'] = psi_hat3  / np.sqrt(np.diag(vcov3))\n",
    "    \n",
    "    # Store all parameters into a .csv file\n",
    "    df_params.to_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\benchmark {company}.csv', index=False)\n",
    "    df_params1.to_csv(data_loc + f'\\\\parameters\\\\model1\\\\params {company}.csv', index=False)\n",
    "    df_params2.to_csv(data_loc + f'\\\\parameters\\\\model2\\\\params {company}.csv', index=False)\n",
    "    df_params3.to_csv(data_loc + f'\\\\parameters\\\\model3\\\\params {company}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Likelihood calculation functionality that can be used to quickly calculate the likelihood given residuals and conditional volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_log_likelihood(sigma2, et):\n",
    "    # QMLE from Franq and Thieu\n",
    "    lls = np.log(sigma2) + ((et ** 2) / sigma2)\n",
    "\n",
    "    # Calculate Quasi Maximum Likelihood\n",
    "    ll = np.nan_to_num(lls).sum()\n",
    "\n",
    "    return -ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, calculate and plot the constructed values of the conditional volatility model, conditional mean model and the distribution of the innovations $z_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_names = ['ARMA-X-apARCH', 'ARMA-apXapARCH', 'ARMA-apGARCHX']\n",
    "\n",
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_params = pd.read_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\benchmark {company}.csv')\n",
    "    df_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\params {company}.csv')\n",
    "    df_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\params {company}.csv')\n",
    "    df_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model\n",
    "    benchmark = ArmaApARCHX(df_total, ['returns'], lags_arma, params=df_params['psi_hat'].tolist())\n",
    "    \n",
    "    \n",
    "    model1 = ArmaXapARCH(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[1]), \n",
    "                         params = df_params1['psi_hat'].tolist())\n",
    "    \n",
    "    model2 = ArmaApARCHX(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[2]), \n",
    "                         params = df_params2['psi_hat'].tolist())\n",
    "    \n",
    "    exog_lags = list(lags_per_model[3])[:len(exog_cols)]\n",
    "    x_garch_lags = list(lags_per_model[3])[len(exog_cols):]\n",
    "    \n",
    "    model3 = ArmaApArchXGarch(df_total, 'returns', lags_arma, exog_cols, exog_lags, \n",
    "                              params = df_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "    \n",
    "    sigma2, et = benchmark.conditional_volatility()\n",
    "    sigma2_1, et_1 = model1.conditional_volatility()\n",
    "    sigma2_2, et_2 = model2.conditional_volatility()\n",
    "    sigma2_3, et_3 = model3.conditional_volatility()\n",
    "    \n",
    "    vars_, ets = [sigma2_1, sigma2_2, sigma2_3], [et_1, et_2, et_3]\n",
    "    \n",
    "    # Calculate likelihood for all models\n",
    "    ll_benchmark = quasi_log_likelihood(sigma2, et)\n",
    "    ll_1, ll_2, ll_3 = quasi_log_likelihood(sigma2_1, et_1), quasi_log_likelihood(sigma2_2, et_2), quasi_log_likelihood(sigma2_3, et_3)\n",
    "    likelihoods = [ll_1, ll_2, ll_3]\n",
    "    \n",
    "    # Now, construct plots for all models\n",
    "    fig, axs = plt.subplots(figsize = (25,5), nrows = 1, ncols = 3)\n",
    "    \n",
    "    first_date, last_date = df_total.date.iloc[0], df_total.date.iloc[-1]\n",
    "    n = 250  # keeps every 150th label (around half a year)\n",
    "\n",
    "    for j in range(len(axs)):\n",
    "        axs[j].plot(df_total.date, np.sqrt(ets[j] ** 2), c='black', linestyle='-.', alpha=0.7, label='Realized volatility')\n",
    "        axs[j].plot(df_total.date, np.sqrt(sigma2), c='yellow', linestyle='--', label='Benchmark model')\n",
    "        axs[j].plot(df_total.date, np.sqrt(vars_[j]), c=colors[0], label=f'Single regime {model_names[j]} model')\n",
    "        \n",
    "        # Set title and xticklabels\n",
    "        axs[j].set_title(f'Conditional volatility of {company}')\n",
    "        axs[j].legend()\n",
    "        \n",
    "        # Set title and xticklabels\n",
    "        axs[j].set_xticks(axs[j].get_xticks()[::n])\n",
    "        axs[j].tick_params(axis='x', labelrotation = 45)\n",
    "        \n",
    "        # Check behaviour of the innovations\n",
    "        #axs[j].hist(ets[j] / np.sqrt(vars_[j]), bins=50)\n",
    "        #print(np.mean(ets[j] / np.sqrt(vars_[j])), np.std(ets[j] / np.sqrt(vars_[j])))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "       \n",
    "    # Store figures as PNG\n",
    "    fig.savefig(store_loc + f'\\\\plots\\\\plot conditional vol {company}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Export parameter DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates and exports the DataFrames with parameter estimates of each model included in this research for all companies in this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create empty DataFrames to store results in\n",
    "df_psi_hat_1 = pd.DataFrame()\n",
    "df_psi_hat_2 = pd.DataFrame()\n",
    "df_psi_hat_3 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df_comp_names['Company'])):\n",
    "    company = df_comp_names['Company'].iloc[i]\n",
    "\n",
    "    # Store all parameters into a .csv file\n",
    "    df_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\params {company}.csv')\n",
    "    df_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\params {company}.csv')\n",
    "    df_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\params {company}.csv')\n",
    "    \n",
    "    # Compile errors in parentheses\n",
    "    errors1 = df_params1['errors'].apply(lambda x: '%.1e' % x).apply(lambda x: f' ({x})')\n",
    "    \n",
    "    # Get p-value based on t-statistic\n",
    "    p = df_params1['t-stat'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)]))    \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params1)):\n",
    "        df_params.loc[len(df_params)] = [df_params1.iloc[j]['param names'], \n",
    "                                         df_params1.iloc[j]['psi_hat'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = ['error_' + df_params1.iloc[j]['param names'], errors1[j]]\n",
    "            \n",
    "    df_params1 = df_params\n",
    "    \n",
    "    # Compile errors in parentheses\n",
    "    errors2 = df_params2['errors'].apply(lambda x: '%.1e' % x).apply(lambda x: f' ({x})')\n",
    "    \n",
    "    # Get p-value based on t-statistic\n",
    "    p = df_params2['t-stat'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)]))\n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params2)):\n",
    "        df_params.loc[len(df_params)] = [df_params2.iloc[j]['param names'], \n",
    "                                         df_params2.iloc[j]['psi_hat'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = ['error_' + df_params2.iloc[j]['param names'], errors2[j]]\n",
    "            \n",
    "    df_params2 = df_params\n",
    "   \n",
    "    # Compile errors in parentheses\n",
    "    errors3 = df_params3['errors'].apply(lambda x: '%.1e' % x).apply(lambda x:  f' ({x})')\n",
    "    \n",
    "    # Get p-value based on t-statistic\n",
    "    p = df_params3['t-stat'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)])) \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params3)):\n",
    "        df_params.loc[len(df_params)] = [df_params3.iloc[j]['param names'], \n",
    "                                         df_params3.iloc[j]['psi_hat'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = ['error_' + df_params3.iloc[j]['param names'], errors3[j]]\n",
    "            \n",
    "    df_params3 = df_params\n",
    "    \n",
    "    if i == 0:        \n",
    "        df_psi_hat_1 = df_params1\n",
    "        df_psi_hat_2 = df_params2\n",
    "        df_psi_hat_3 = df_params3\n",
    "    else:\n",
    "        df_psi_hat_1 = df_psi_hat_1.merge(df_params1[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer') \n",
    "\n",
    "        df_psi_hat_2 = df_psi_hat_2.merge(df_params2[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "    \n",
    "\n",
    "        df_psi_hat_3 = df_psi_hat_3.merge(df_params3[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "    \n",
    "    # Save DataFrames to .csv\n",
    "    df_psi_hat_1.to_csv(data_loc + f'\\\\parameters\\\\params model1.csv')\n",
    "    df_psi_hat_2.to_csv(data_loc + f'\\\\parameters\\\\params model2.csv')\n",
    "    df_psi_hat_3.to_csv(data_loc + f'\\\\parameters\\\\params model3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
