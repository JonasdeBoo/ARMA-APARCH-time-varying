{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3. Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the results of the various GARCH models that used exogenous regressors are displayed. Both the time-varying models as well as the regular models are shown in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1. Load packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, load the packages, data and colors for the main analysis and for the construction of plots, also define export locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "import os, sys\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load data that returns tweets\n",
    "sys.path.insert(0, os.path.abspath(r'C:\\Users\\Jonas\\PycharmProjects\\TwitterSentimentGARCH2021\\Code\\GARCH model'))\n",
    "from garch_models import ArmaApARCHX, ArmaXapARCH, ArmaApArchXGarch, ExogGarch\n",
    "sys.path.insert(0, os.path.abspath(r'C:\\Users\\Jonas\\PycharmProjects\\TwitterSentimentGARCH2021\\Code\\GARCH model\\tvGARCH models'))\n",
    "from tv_garch_models import tvArmaApARCHX, tvArmaXapARCH, tvArmaApArchXGarch\n",
    "\n",
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1.1. Colors for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['seagreen', 'mediumaquamarine', 'steelblue', 'cornflowerblue', 'navy', 'black'] * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1.2 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load company name data and DataFrame per company containing all the sentiment, return and control variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of data + file name and location of storage\n",
    "company_loc = r'C:\\Users\\Jonas\\Documents\\Data'\n",
    "file_name_comp = '\\company_ticker_list_all.xlsx'\n",
    "\n",
    "# Access company names DataFrame\n",
    "df_comp_names = pd.read_excel(company_loc + file_name_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify location where all company specific data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify name and location\n",
    "data_loc = r'C:\\Users\\Jonas\\Documents\\Data\\Total_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify location where to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location where daily sentiment scores must be stored\n",
    "store_loc = r'C:\\Users\\Jonas\\Documents\\Data\\Results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard code the numerous types of columns used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible exogenous columns\n",
    "control_cols = ['VIX', 'TEDRATE']\n",
    "sent_cols = ['sentiment', 'n_tweets', 'n_interactions']\n",
    "\n",
    "exog_cols = sent_cols\n",
    "\n",
    "# Define data on which to impose GARCH structure, and provide column names\n",
    "x_garch_cols = [f'sigma2_{col}' for col in sent_cols]\n",
    "\n",
    "# Define proper model names used for plots\n",
    "model_names = ['ARMA-X-apARCH', 'ARMA-apXapARCH', 'ARMA-apGARCHX']\n",
    "\n",
    "# Define tv smoothing operator h\n",
    "h = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function that can calculate the quasi log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_log_likelihood(sigma2, et):\n",
    "    # QMLE from Franq and Thieu\n",
    "    lls = np.log(sigma2) + ((et ** 2) / sigma2)\n",
    "\n",
    "    # Calculate Quasi Maximum Likelihood\n",
    "    ll = np.nan_to_num(lls).sum()\n",
    "\n",
    "    return -ll\n",
    "\n",
    "def likelihoodratio(ll_1, ll_2):\n",
    "    return - 2 * (ll_1 - ll_2)\n",
    "\n",
    "def bic(n, T, LL):\n",
    "    return n * np.log(T) - (2 * LL * T)\n",
    "\n",
    "import colorsys\n",
    "\n",
    "def scale_lightness(rgb, scale_l):\n",
    "    # convert rgb to hls\n",
    "    h, l, s = colorsys.rgb_to_hls(*rgb)\n",
    "    # manipulate h, l, s values and return as rgb\n",
    "    return colorsys.hls_to_rgb(h, min(1, l * scale_l), s = s)\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "color = matplotlib.colors.ColorConverter.to_rgb(\"navy\")\n",
    "rgbs = [scale_lightness(color, scale) for scale in [0, .5, 1, 1.5, 2]]\n",
    "navy_color = rgbs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2. Summary of exogenous GARCH process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will shown the descriptive statistics and parameters of the Exogenous GARCH process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for i, company in enumerate(df_comp_names['Company']):   \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total, with sigma2 in there\n",
    "    df_total = pd.read_csv(data_loc + data_name)   \n",
    "    \n",
    "    fig, axs = plt.subplots(figsize = (30,8), nrows = 1, ncols = 6)\n",
    "    \n",
    "    for j, col in enumerate(sent_cols):         \n",
    "        # Append df_total with the sigma2_x variable\n",
    "        sigma2_x = df_total[f'sigma2_{col}']\n",
    "        et_x = df_total[f'et_{col}']\n",
    "        j = 2 * j\n",
    "        plot_pacf(df_total[col], ax=axs[j], lags=np.arange(10), alpha=0.05, method='ols', title=f'Partial autocorrelation of {col}', zero=False)\n",
    "        j += 1\n",
    "        plot_pacf(et_x, ax=axs[j], lags=np.arange(10), alpha=0.05, method='ols', title=f'Partial autocorrelation of et_{col}', zero=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the exogenous GARCH process and show descriptive statistics and parameters of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for i, company in enumerate(df_comp_names['Company']):   \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total, with sigma2 in there\n",
    "    df_total = pd.read_csv(data_loc + data_name)   \n",
    "    \n",
    "    # Read parameters of exogenous GARCH process\n",
    "    df_xgarch_params = pd.read_csv(data_loc + f'\\\\exogenous garch data\\\\xgarch_params {company}.csv')\n",
    "    \n",
    "    # Split df_xgarch_params into three subdataframes\n",
    "    df_params1 = df_xgarch_params.iloc[:, 1:5]\n",
    "    df_params2 = df_xgarch_params.iloc[:, 5:9]\n",
    "    df_params3 = df_xgarch_params.iloc[:, 9:13]\n",
    "    \n",
    "    # Create summary statistics\n",
    "    ds_sentiment1 = [[['mean', np.mean(df_total[col])], \n",
    "                     ['std',   np.std(df_total[col])], \n",
    "                     ['n_obs', len(df_total[col])],\n",
    "                     ['min',   np.min(df_total[col])],  \n",
    "                     ['max',   np.max(df_total[col])],\n",
    "                     ['QLL', quasi_log_likelihood(df_total[f'sigma2_{col}'], df_total[f'et_{col}'])]] for col in sent_cols]\n",
    "    \n",
    "    ds_sentiment2 = [[[np.mean(df_total[col])], \n",
    "                     [np.std(df_total[col])], \n",
    "                     [len(df_total[col])],\n",
    "                     [np.min(df_total[col])],  \n",
    "                     [np.max(df_total[col])],\n",
    "                     [quasi_log_likelihood(df_total[f'sigma2_{col}'], df_total[f'et_{col}'])]] for col in sent_cols]\n",
    "    \n",
    "    # Compile errors in parentheses\n",
    "    errors1 = df_params1['errors'].apply(lambda x: '%.1e' % x).apply(lambda x: f' ({x})')\n",
    "    p = df_params1['t-stat'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-2*alpha)]))\n",
    "    \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    for j in range(len(df_params1)):\n",
    "        df_params.loc[len(df_params)] = [df_params1.iloc[j]['param names'], \n",
    "                                         df_params1.iloc[j]['psi_hat'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = [f'error {j}', errors1[j]]\n",
    "            \n",
    "    df_params1 = df_params\n",
    "\n",
    "    \n",
    "    errors2 = df_params2['errors.1'].apply(lambda x: '%.1e' % x).apply(lambda x: f' ({x})')\n",
    "    p = df_params2['t-stat.1'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)]))\n",
    "    \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    for j in range(len(df_params2)):\n",
    "        df_params.loc[len(df_params)] = [df_params2.iloc[j]['param names.1'], \n",
    "                                         df_params2.iloc[j]['psi_hat.1'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = [f'error {j}', errors2[j]]\n",
    "            \n",
    "    df_params2 = df_params\n",
    "   \n",
    "    \n",
    "    errors3 = df_params3['errors.2'].apply(lambda x: '%.1e' % x).apply(lambda x:  f' ({x})')\n",
    "    p = df_params3['t-stat.2'].apply(lambda x: ''.join(['*' for alpha in [0.1, 0.05, 0.01] if abs(x) >= norm.ppf(1-alpha)]))\n",
    "    \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    for j in range(len(df_params3)):\n",
    "        df_params.loc[len(df_params)] = [df_params3.iloc[j]['param names.2'], \n",
    "                                         df_params3.iloc[j]['psi_hat.2'].round(3).astype(str) + p[j]]\n",
    "        df_params.loc[len(df_params)] = [f'error {j}', errors3[j]]\n",
    "            \n",
    "    df_params3 = df_params\n",
    "    \n",
    "    df_params1 = df_params1.append(pd.DataFrame(ds_sentiment1[0], columns=df_params1.columns)).reset_index(drop=True)\n",
    "    df_params2 = df_params2.append(pd.DataFrame(ds_sentiment1[1], columns=df_params2.columns)).reset_index(drop=True)\n",
    "    df_params3 = df_params3.append(pd.DataFrame(ds_sentiment1[2], columns=df_params3.columns)).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    if i == 0:        \n",
    "        df_psi_hat_1 = df_params1\n",
    "        df_psi_hat_2 = df_params2\n",
    "        df_psi_hat_3 = df_params3\n",
    "    else:\n",
    "        df_psi_hat_1 = df_psi_hat_1.merge(df_params1[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer') \n",
    "\n",
    "        df_psi_hat_2 = df_psi_hat_2.merge(df_params2[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "    \n",
    "\n",
    "        df_psi_hat_3 = df_psi_hat_3.merge(df_params3[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "     \n",
    "    # Save DataFrames to .csv\n",
    "    df_psi_hat_1.to_csv(data_loc + f'\\\\parameters\\\\params exog {sent_cols[0]}.csv')\n",
    "    df_psi_hat_2.to_csv(data_loc + f'\\\\parameters\\\\params exog {sent_cols[1]}.csv')\n",
    "    df_psi_hat_3.to_csv(data_loc + f'\\\\parameters\\\\params exog {sent_cols[2]}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots of the exogenous processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company in df_comp_names['Company']:   \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)        \n",
    "    \n",
    "    # Construct plots for all sentiment cols\n",
    "    fig, axs = plt.subplots(figsize = (20,4), nrows = 1, ncols = 3)\n",
    "\n",
    "    first_date, last_date = df_total.date.iloc[0], df_total.date.iloc[-1]\n",
    "    n = 250  # keeps every 250th label (around a year)\n",
    "\n",
    "    for j, col in enumerate(sent_cols):         \n",
    "        # Append df_total with the sigma2_x variable\n",
    "        sigma2_x = df_total[f'sigma2_{col}']\n",
    "        et_x = df_total[f'et_{col}']\n",
    "\n",
    "        # Plot volatility of sentiment metric\n",
    "        axs[j].plot(df_total.date, np.sqrt(et_x ** 2), c=navy_color, linestyle='--')\n",
    "        axs[j].plot(df_total.date, np.sqrt(sigma2_x), c=colors[j])\n",
    "        \n",
    "        # Set title and xticklabels\n",
    "        axs[j].set_xticks(axs[j].get_xticks()[::n])\n",
    "        axs[j].tick_params(axis='x', labelrotation = 45)\n",
    "        \n",
    "        # Check behaviour of the innovations\n",
    "        #axs[j].hist(et_x / np.sqrt(sigma2_x), bins=50)\n",
    "        #print(np.mean(et_x / np.sqrt(sigma2_x)), np.std(et_x / np.sqrt(sigma2_x)))\n",
    "\n",
    "        # Set title and xticklabels\n",
    "        axs[j].set_title(f'Conditional volatility of {col} of {company}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    # Store figures as PNG\n",
    "    fig.savefig(store_loc + f'\\\\plots\\\\plots sigma2 sentiment {company}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 Summary of results: plots of volatility processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, calculate and plot the constructed values of the conditional volatility model, by any of the models, including the benchmark and the time-varying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company, ticker in zip(df_comp_names['Company'], df_comp_names['Symbol']):    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Calculate realised volatility\n",
    "    returns_name = f'//{ticker}.csv'\n",
    "    df_returns = pd.read_csv(f'C:\\\\Users\\\\Jonas\\\\Documents\\\\Data\\\\Returns\\\\{ticker}.csv')   \n",
    "    adjusted_range = (1 / (2 * np.sqrt(np.log(2)))) * (df_returns.High[1:] - df_returns.Low[1:]) # Calculate adjusted range\n",
    "    df_total['real_vol'] = adjusted_range\n",
    "    \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    exog_lags = list(lags_per_model[3])[:len(exog_cols)]\n",
    "    x_garch_lags = list(lags_per_model[3])[len(exog_cols):]\n",
    "    \n",
    "    ### ---------- SINGLE REGIME SPECIFICATION --------- ###\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_params = pd.read_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\benchmark {company}.csv')\n",
    "    df_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\params {company}.csv')\n",
    "    df_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\params {company}.csv')\n",
    "    df_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model\n",
    "    benchmark = ArmaApARCHX(df_total, 'returns', lags_arma, params=df_params['psi_hat'].tolist())\n",
    "    \n",
    "    \n",
    "    model1 = ArmaXapARCH(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[1]), \n",
    "                         params = df_params1['psi_hat'].tolist())\n",
    "    \n",
    "    model2 = ArmaApARCHX(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[2]), \n",
    "                         params = df_params2['psi_hat'].tolist())\n",
    "       \n",
    "    model3 = ArmaApArchXGarch(df_total, 'returns', lags_arma, exog_cols, exog_lags, \n",
    "                              params = df_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "    \n",
    "    sigma2, et = benchmark.conditional_volatility()\n",
    "    sigma2_1, et_1 = model1.conditional_volatility()\n",
    "    sigma2_2, et_2 = model2.conditional_volatility()\n",
    "    sigma2_3, et_3 = model3.conditional_volatility()\n",
    "    \n",
    "    vars_, ets = [sigma2_1, sigma2_2, sigma2_3], [et_1, et_2, et_3]\n",
    "     \n",
    "    ### ---------- TIME VARYING SPECIFICATION --------- ###\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_tv_params = pd.read_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\tv params {company}.csv')\n",
    "    df_tv_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\tv params {company}.csv')\n",
    "    df_tv_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\tv params {company}.csv')\n",
    "    df_tv_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\tv params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model\n",
    "    tv_benchmark = tvArmaApARCHX(df_total, 'returns', control_cols, h, lags_arma, params=df_tv_params['psi_hat'].tolist())\n",
    "    \n",
    "    \n",
    "    tv_model1 = tvArmaXapARCH(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[1]), \n",
    "                         params = df_tv_params1['psi_hat'].tolist())\n",
    "    \n",
    "    tv_model2 = tvArmaApARCHX(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[2]), \n",
    "                         params = df_tv_params2['psi_hat'].tolist())\n",
    "    \n",
    "    tv_model3 = tvArmaApArchXGarch(df_total, 'returns', control_cols, h, lags_arma, sent_cols, exog_lags, \n",
    "                              params = df_tv_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "    \n",
    "    sigma2_tv, et_tv = tv_benchmark.conditional_volatility()\n",
    "    sigma2_tv1, et_tv1 = tv_model1.conditional_volatility()\n",
    "    sigma2_tv2, et_tv2 = tv_model2.conditional_volatility()\n",
    "    sigma2_tv3, et_tv3 = tv_model3.conditional_volatility()\n",
    "    \n",
    "    vars_tv, ets_tv = [sigma2_tv1, sigma2_tv2, sigma2_tv3], [et_tv1, et_tv2, et_tv3]\n",
    "    \n",
    "    ### --------------- RENDERING PLOTS ------------------ ###\n",
    "       \n",
    "    # Now, construct plots for all models\n",
    "    fig, axs = plt.subplots(figsize = (20,5), nrows = 1, ncols = 3, sharey=True)\n",
    "    \n",
    "    first_date, last_date = df_total.date.iloc[0], df_total.date.iloc[-1]\n",
    "    n = 250  # keeps every 150th label (around half a year)\n",
    "\n",
    "    for j in range(len(axs)):\n",
    "        axs[j].plot(df_total.date, np.sqrt(et ** 2), c=navy_color, linestyle='-.', label='Squared ARMA residuals')\n",
    "        axs[j].plot(df_total.date, np.sqrt(sigma2), c='yellow', linewidth=2, linestyle='--', label='Benchmark model')\n",
    "        axs[j].plot(df_total.date, np.sqrt(vars_[j]), c='firebrick', linewidth=2, alpha=0.8, label=f'Single regime {model_names[j]} model')\n",
    "        axs[j].plot(df_total.date, np.sqrt(vars_tv[j]), c=colors[1], linewidth=1.7, linestyle='--', label=f'Time-varying {model_names[j]} model')\n",
    "        \n",
    "        # Set title and xticklabels\n",
    "        axs[j].legend(loc='upper left', fontsize=10)\n",
    "        \n",
    "        # Set title and xticklabels\n",
    "        axs[j].set_xticks(axs[j].get_xticks()[::n])\n",
    "        axs[j].tick_params(axis='x', labelrotation = 45)\n",
    "        \n",
    "        # Check behaviour of the innovations\n",
    "        #axs[j].hist(ets[j] / np.sqrt(vars_[j]), bins=50)\n",
    "        #print(np.mean(ets[j] / np.sqrt(vars_[j])), np.std(ets[j] / np.sqrt(vars_[j])))\n",
    "    \n",
    "    plt.suptitle(f'Conditional volatility of {company} using multiple apARCH extensions', size=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Store figures as PNG\n",
    "    fig.savefig(store_loc + f'\\\\plots\\\\plot conditional vol {company}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2. Summary of results: QLR statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates and exports the DataFrames with parameter estimates of each model included in this research for all companies in this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define DataFrame with LR statistics\n",
    "cols = ['benchmark'] + model_names + [f'tv {model}' for model in model_names]\n",
    "df_stats = pd.DataFrame(columns = cols)\n",
    "\n",
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company, ticker in zip(df_comp_names['Company'], df_comp_names['Symbol']):    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "       \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    exog_lags = list(lags_per_model[3])[:len(exog_cols)]\n",
    "    x_garch_lags = list(lags_per_model[3])[len(exog_cols):]\n",
    "    \n",
    "    ### ---------- SINGLE REGIME SPECIFICATION --------- ###\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_params = pd.read_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\benchmark {company}.csv')\n",
    "    df_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\params {company}.csv')\n",
    "    df_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\params {company}.csv')\n",
    "    df_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model\n",
    "    benchmark = ArmaApARCHX(df_total, 'returns', lags_arma, params=df_params['psi_hat'].tolist())\n",
    "    \n",
    "    \n",
    "    model1 = ArmaXapARCH(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[1]), \n",
    "                         params = df_params1['psi_hat'].tolist())\n",
    "    \n",
    "    model2 = ArmaApARCHX(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[2]), \n",
    "                         params = df_params2['psi_hat'].tolist())\n",
    "       \n",
    "    model3 = ArmaApArchXGarch(df_total, 'returns', lags_arma, exog_cols, exog_lags, \n",
    "                              params = df_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "    \n",
    "    sigma2, et = benchmark.conditional_volatility()\n",
    "    sigma2_1, et_1 = model1.conditional_volatility()\n",
    "    sigma2_2, et_2 = model2.conditional_volatility()\n",
    "    sigma2_3, et_3 = model3.conditional_volatility()\n",
    "    \n",
    "     \n",
    "    ### ---------- TIME VARYING SPECIFICATION --------- ###\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_tv_params = pd.read_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\tv params {company}.csv')\n",
    "    df_tv_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\tv params {company}.csv')\n",
    "    df_tv_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\tv params {company}.csv')\n",
    "    df_tv_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\tv params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model\n",
    "    tv_benchmark = tvArmaApARCHX(df_total, 'returns', control_cols, h, lags_arma, params=df_tv_params['psi_hat'].tolist())\n",
    "    \n",
    "    \n",
    "    tv_model1 = tvArmaXapARCH(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[1]), \n",
    "                         params = df_tv_params1['psi_hat'].tolist())\n",
    "    \n",
    "    tv_model2 = tvArmaApARCHX(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[2]), \n",
    "                         params = df_tv_params2['psi_hat'].tolist())\n",
    "    \n",
    "    tv_model3 = tvArmaApArchXGarch(df_total, 'returns', control_cols, h, lags_arma, sent_cols, exog_lags, \n",
    "                              params = df_tv_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "    \n",
    "    sigma2_tv, et_tv = tv_benchmark.conditional_volatility()\n",
    "    sigma2_tv1, et_tv1 = tv_model1.conditional_volatility()\n",
    "    sigma2_tv2, et_tv2 = tv_model2.conditional_volatility()\n",
    "    sigma2_tv3, et_tv3 = tv_model3.conditional_volatility()    \n",
    "       \n",
    "    ### -------------- CALCULATING LR STATISTICS ------------------ ###\n",
    "    \n",
    "    # Calculate number of parameters\n",
    "    n_benchmark = len(df_params['psi_hat'])\n",
    "    n1, n2, n3 = len(df_params1['psi_hat']), len(df_params2['psi_hat']), len(df_params3['psi_hat'])\n",
    "    n1_tv, n2_tv, n3_tv = len(df_tv_params1['psi_hat']), len(df_tv_params2['psi_hat']), len(df_tv_params3['psi_hat'])\n",
    "    \n",
    "    # Calculate likelihood for all models\n",
    "    ll_benchmark = quasi_log_likelihood(sigma2, et)\n",
    "    ll_1, ll_2, ll_3 = quasi_log_likelihood(sigma2_1, et_1), quasi_log_likelihood(sigma2_2, et_2), quasi_log_likelihood(sigma2_3, et_3)\n",
    "    likelihoods = [ll_1, ll_2, ll_3]\n",
    "\n",
    "    # Calculate likelihood for all models\n",
    "    ll_benchmark_tv = quasi_log_likelihood(sigma2_tv, et_tv)\n",
    "    ll_tv1, ll_tv2, ll_tv3 = quasi_log_likelihood(sigma2_tv1, et_tv1), quasi_log_likelihood(sigma2_tv2, et_tv2), quasi_log_likelihood(sigma2_tv3, et_tv3)\n",
    "    likelihoods_tv = [ll_tv1, ll_tv2, ll_tv3]\n",
    "    \n",
    "    # LR with benchmark\n",
    "    lr_1, lr_2, lr_3, lr_4, lr_5, lr_6 = likelihoodratio(ll_benchmark, ll_1), likelihoodratio(ll_benchmark, ll_2), likelihoodratio(ll_benchmark, ll_3), likelihoodratio(ll_benchmark, ll_tv1), likelihoodratio(ll_benchmark, ll_tv2), likelihoodratio(ll_benchmark, ll_tv3)\n",
    "    \n",
    "    # LR with time-varying parameters\n",
    "    lr_tv1, lr_tv2, lr_tv3 = likelihoodratio(ll_1, ll_tv1), likelihoodratio(ll_2, ll_tv2), likelihoodratio(ll_3, ll_tv3)\n",
    "    \n",
    "    # LR allowing conditional vol of exogenous variables to enter model\n",
    "    lr_lambda1, lr_lambda2 = likelihoodratio(ll_2, ll_3), likelihoodratio(ll_tv2, ll_tv3)\n",
    "    \n",
    "    # Define rows with LLs and LR statistics\n",
    "    row1 = [ll_benchmark, ll_1, ll_2, ll_3, ll_tv1, ll_tv2, ll_tv3]\n",
    "    row2 = [0, lr_1.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_1 >= chi2.ppf(1-alpha, n1-n_benchmark)]), \n",
    "            lr_2.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_2 >= chi2.ppf(1-alpha, n2-n_benchmark)]), \n",
    "            lr_3.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_3 >= chi2.ppf(1-alpha, n3-n_benchmark)]), \n",
    "            lr_4.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_4 >= chi2.ppf(1-alpha, n1_tv-n_benchmark)]), \n",
    "            lr_5.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_5 >= chi2.ppf(1-alpha, n2_tv-n_benchmark)]), \n",
    "            lr_6.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_6 >= chi2.ppf(1-alpha, n3_tv-n_benchmark)])]\n",
    "    row3 = [0, 0, 0, 0,\n",
    "            lr_tv1.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_tv1 >= chi2.ppf(1-alpha, n1_tv-n1)]),\n",
    "            lr_tv2.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_tv2 >= chi2.ppf(1-alpha, n2_tv-n2)]),\n",
    "            lr_tv3.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_tv3 >= chi2.ppf(1-alpha, n3_tv-n3)])]\n",
    "    row4 = [0, 0, \n",
    "            lr_lambda1.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_lambda1 >= chi2.ppf(1-alpha, n3-n2)]), \n",
    "            0, 0, \n",
    "            lr_lambda2.round(3).astype(str) + ''.join(['*' for alpha in [0.1, 0.05, 0.01] if lr_lambda2 >= chi2.ppf(1-alpha, n3_tv-n2_tv)]), \n",
    "            0]\n",
    "    row5 = [bic(len(df_params['psi_hat'].tolist()), len(df_total), ll_benchmark),\n",
    "            bic(len(df_params1['psi_hat'].tolist()), len(df_total), ll_1),\n",
    "            bic(len(df_params2['psi_hat'].tolist()), len(df_total), ll_2),\n",
    "            bic(len(df_params3['psi_hat'].tolist()), len(df_total), ll_3),\n",
    "            bic(len(df_tv_params1['psi_hat'].tolist()), len(df_total), ll_tv1),\n",
    "            bic(len(df_tv_params2['psi_hat'].tolist()), len(df_total), ll_tv2),\n",
    "            bic(len(df_tv_params3['psi_hat'].tolist()), len(df_total), ll_tv3),]\n",
    "    \n",
    "    # Append created statistics to DataFrame\n",
    "    row1 = ['%.3e' % np.round(x) for x in row1]\n",
    "    row5 = ['%.3e' % np.round(x) for x in row5]\n",
    "       \n",
    "    df_stats.loc[len(df_stats)] = row1\n",
    "    df_stats.loc[len(df_stats)] = row5\n",
    "    df_stats.loc[len(df_stats)] = row2\n",
    "    df_stats.loc[len(df_stats)] = row3\n",
    "    df_stats.loc[len(df_stats)] = row4\n",
    "    \n",
    "    \n",
    "# Construct index of LL/LR table\n",
    "iterables = [[company for company in df_comp_names['Company']], [\"QLR statistic\", \"BIC\", \"LR vs benchmark\", \"LR single- vs multiregime\", \"lambda = 0\"]]\n",
    "df_stats.index = pd.MultiIndex.from_product(iterables, names=[\"first\", \"second\"])    \n",
    "\n",
    "# Display df_stats\n",
    "display(df_stats)\n",
    "\n",
    "# Save stats \n",
    "df_stats.to_csv(store_loc + '//table model statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 Summary of results: in sample fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, tables displaying the in-sample fit of the models are calculated. This is done using the MAPE, the QLIKE and the mincer-zarkowitz regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataFrame with LR statistics\n",
    "indx = ['benchmark'] + [f'Model {i}' for i in range(1, len(model_names)+1)] + [f'tv Model {i}' for i in range(1, len(model_names)+1)]\n",
    "df_mse = pd.DataFrame(index=indx)\n",
    "df_qlike = pd.DataFrame(index=indx)\n",
    "\n",
    "# Find lags specification for each model\n",
    "df_model_specification = pd.read_csv(data_loc + f'\\\\lags\\\\model_params.csv')\n",
    "\n",
    "# For every company in the dataset, read the data and construct the appropriate model\n",
    "for company, ticker in zip(df_comp_names['Company'], df_comp_names['Symbol']):    \n",
    "    # Read data\n",
    "    data_name = f'\\\\total data {company}.csv' \n",
    "    \n",
    "    # Get df_total\n",
    "    df_total = pd.read_csv(data_loc + data_name)\n",
    "       \n",
    "    # Drop NaNs\n",
    "    df_total = df_total.fillna(0)\n",
    "    \n",
    "    # Calculate realised volatility\n",
    "    returns_name = f'//{ticker}.csv'\n",
    "    df_returns = pd.read_csv(f'C:\\\\Users\\\\Jonas\\\\Documents\\\\Data\\\\Returns\\\\{ticker}.csv')   \n",
    "    adjusted_range = 0.6006 * (df_returns.High[1:] - df_returns.Low[1:]) # Calculate adjusted range\n",
    "    df_total['real_vol'] = adjusted_range\n",
    "       \n",
    "    # Unpack the optimal lags of the ARMA procedure from df_model_specification\n",
    "    lags_per_model = [literal_eval(x) for x in df_model_specification[company]]\n",
    "    lags_arma = list(lags_per_model[0])\n",
    "    exog_lags = list(lags_per_model[3])[:len(exog_cols)]\n",
    "    x_garch_lags = list(lags_per_model[3])[len(exog_cols):]\n",
    "    \n",
    "    ### ---------- SINGLE REGIME SPECIFICATION --------- ###\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_params = pd.read_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\benchmark {company}.csv')\n",
    "    df_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\params {company}.csv')\n",
    "    df_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\params {company}.csv')\n",
    "    df_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model\n",
    "    benchmark = ArmaApARCHX(df_total, 'returns', lags_arma, params=df_params['psi_hat'].tolist())\n",
    "    \n",
    "    \n",
    "    model1 = ArmaXapARCH(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[1]), \n",
    "                         params = df_params1['psi_hat'].tolist())\n",
    "    \n",
    "    model2 = ArmaApARCHX(df_total, 'returns', lags_arma, exog_cols, list(lags_per_model[2]), \n",
    "                         params = df_params2['psi_hat'].tolist())\n",
    "       \n",
    "    model3 = ArmaApArchXGarch(df_total, 'returns', lags_arma, exog_cols, exog_lags, \n",
    "                              params = df_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "    \n",
    "    sigma2, et = benchmark.conditional_volatility()\n",
    "    sigma2_1, et_1 = model1.conditional_volatility()\n",
    "    sigma2_2, et_2 = model2.conditional_volatility()\n",
    "    sigma2_3, et_3 = model3.conditional_volatility()\n",
    "    \n",
    "     \n",
    "    ### ---------- TIME VARYING SPECIFICATION --------- ###\n",
    "    \n",
    "    # Open parameter files\n",
    "    df_tv_params = pd.read_csv(data_loc + f'\\\\parameters\\\\benchmark\\\\tv params {company}.csv')\n",
    "    df_tv_params1 = pd.read_csv(data_loc + f'\\\\parameters\\\\model1\\\\tv params {company}.csv')\n",
    "    df_tv_params2 = pd.read_csv(data_loc + f'\\\\parameters\\\\model2\\\\tv params {company}.csv')\n",
    "    df_tv_params3 = pd.read_csv(data_loc + f'\\\\parameters\\\\model3\\\\tv params {company}.csv')\n",
    "    \n",
    "    # Calculate sigma2 values for each model\n",
    "    tv_benchmark = tvArmaApARCHX(df_total, 'returns', control_cols, h, lags_arma, params=df_tv_params['psi_hat'].tolist())\n",
    "    \n",
    "    \n",
    "    tv_model1 = tvArmaXapARCH(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[1]), \n",
    "                         params = df_tv_params1['psi_hat'].tolist())\n",
    "    \n",
    "    tv_model2 = tvArmaApARCHX(df_total, 'returns', control_cols, h, lags_arma, sent_cols, list(lags_per_model[2]), \n",
    "                         params = df_tv_params2['psi_hat'].tolist())\n",
    "    \n",
    "    tv_model3 = tvArmaApArchXGarch(df_total, 'returns', control_cols, h, lags_arma, sent_cols, exog_lags, \n",
    "                              params = df_tv_params3['psi_hat'].tolist(),\n",
    "                              xgarch_cols=x_garch_cols, lag_exog_sigma=x_garch_lags)\n",
    "    \n",
    "    sigma2_tv, et_tv = tv_benchmark.conditional_volatility()\n",
    "    sigma2_tv1, et_tv1 = tv_model1.conditional_volatility()\n",
    "    sigma2_tv2, et_tv2 = tv_model2.conditional_volatility()\n",
    "    sigma2_tv3, et_tv3 = tv_model3.conditional_volatility()    \n",
    "       \n",
    "    ### -------------- CALCULATING IN-SAMPLE FIT COMPARISON ------------------ ###\n",
    "    sigma2_proxy = et ** 2\n",
    "    sigma2_pred = [sigma2, sigma2_1, sigma2_2, sigma2_3, sigma2_tv1, sigma2_tv2, sigma2_tv3]\n",
    "    \n",
    "    MSE = [np.mean((sigma2_proxy - sigma) ** 2) for sigma in sigma2_pred]\n",
    "    \n",
    "    QLIKE = [np.mean(np.log(sigma) + (sigma2_proxy / sigma)) for sigma in sigma2_pred]\n",
    "    \n",
    "    df_mse[ticker] = MSE\n",
    "    df_qlike[ticker] = QLIKE\n",
    "    \n",
    "df_insample = pd.concat([df_mse, df_qlike]).round(4)\n",
    "    \n",
    "df_insample.to_csv(store_loc + '//insample_fit_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_insample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check decrease/increase in persistence when allowing for time-varying parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = r'C:\\Users\\Jonas\\Documents\\Data\\Total_data'\n",
    "\n",
    "# Create empty DataFrames to store results in\n",
    "df_params1 = pd.DataFrame()\n",
    "df_params2 = pd.DataFrame()\n",
    "df_params3 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df_comp_names['Company'])):\n",
    "    company = df_comp_names['Company'].iloc[i]\n",
    "\n",
    "    # Store all parameters into a .csv file\n",
    "    df_params_1 = pd.read_csv(loc + f'\\\\parameters\\\\model1\\\\params {company}.csv')\n",
    "    df_params_2 = pd.read_csv(loc + f'\\\\parameters\\\\model2\\\\params {company}.csv')\n",
    "    df_params_3 = pd.read_csv(loc + f'\\\\parameters\\\\model3\\\\params {company}.csv')\n",
    "      \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params_1)):\n",
    "        df_params.loc[len(df_params)] = [df_params_1.iloc[j]['param names'], \n",
    "                                         df_params_1.iloc[j]['psi_hat'].round(3)]\n",
    "            \n",
    "    df_params_1 = df_params\n",
    "\n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params_2)):\n",
    "        df_params.loc[len(df_params)] = [df_params_2.iloc[j]['param names'], \n",
    "                                         df_params_2.iloc[j]['psi_hat'].round(3)]\n",
    "            \n",
    "    df_params_2 = df_params\n",
    "   \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params_3)):\n",
    "        df_params.loc[len(df_params)] = [df_params_3.iloc[j]['param names'], \n",
    "                                         df_params_3.iloc[j]['psi_hat'].round(3)]\n",
    "            \n",
    "    df_params_3 = df_params\n",
    "    \n",
    "    if i == 0:        \n",
    "        df_params1 = df_params_1\n",
    "        df_params2 = df_params_2\n",
    "        df_params3 = df_params_3\n",
    "    else:\n",
    "        df_params1 = df_params1.merge(df_params_1[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer') \n",
    "\n",
    "        df_params2 = df_params2.merge(df_params_2[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "    \n",
    "\n",
    "        df_params3 = df_params3.merge(df_params_3[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "\n",
    "\n",
    "# Create empty DataFrames to store results in\n",
    "df_tv_params1 = pd.DataFrame()\n",
    "df_tv_params2 = pd.DataFrame()\n",
    "df_tv_params3 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df_comp_names['Company'])):\n",
    "    company = df_comp_names['Company'].iloc[i]\n",
    "\n",
    "    # Store all parameters into a .csv file\n",
    "    df_params_1 = pd.read_csv(loc + f'\\\\parameters\\\\model1\\\\tv params {company}.csv')\n",
    "    df_params_2 = pd.read_csv(loc + f'\\\\parameters\\\\model2\\\\tv params {company}.csv')\n",
    "    df_params_3 = pd.read_csv(loc + f'\\\\parameters\\\\model3\\\\tv params {company}.csv')\n",
    "    \n",
    "    # Compile errors in parentheses    \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params_1)):\n",
    "        df_params.loc[len(df_params)] = [df_params_1.iloc[j]['param names'], \n",
    "                                         df_params_1.iloc[j]['psi_hat'].round(3)]\n",
    "            \n",
    "    df_params_1 = df_params\n",
    "    \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params_2)):\n",
    "        df_params.loc[len(df_params)] = [df_params_2.iloc[j]['param names'], \n",
    "                                         df_params_2.iloc[j]['psi_hat'].round(3)]\n",
    "            \n",
    "    df_params_2 = df_params\n",
    "   \n",
    "    df_params = pd.DataFrame(columns = ['param names', f'psi_hat_{company}'])\n",
    "    \n",
    "    # Create DataFrame with errors in brackets under estimates, starts indicating the significance\n",
    "    for j in range(len(df_params_3)):\n",
    "        df_params.loc[len(df_params)] = [df_params_3.iloc[j]['param names'], \n",
    "                                         df_params_3.iloc[j]['psi_hat'].round(3)]\n",
    "            \n",
    "    df_params_3 = df_params\n",
    "    \n",
    "    if i == 0:        \n",
    "        df_tv_params1 = df_params_1\n",
    "        df_tv_params2 = df_params_2\n",
    "        df_tv_params3 = df_params_3\n",
    "    else:\n",
    "        df_tv_params1 = df_tv_params1.merge(df_params_1[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer') \n",
    "\n",
    "        df_tv_params2 = df_tv_params2.merge(df_params_2[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')\n",
    "    \n",
    "\n",
    "        df_tv_params3 = df_tv_params3.merge(df_params_3[['param names', f'psi_hat_{company}']], \n",
    "                                          on='param names', \n",
    "                                          how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persistence = pd.DataFrame(columns = [f'psi_hat_{company}' for company in df_comp_names['Company']])\n",
    "\n",
    "for df in [df_params1, df_tv_params1, df_params2, df_tv_params2, df_params3, df_tv_params3]:   \n",
    "    param_names = df['param names']\n",
    "    alpha_ind = [i for i, elem in enumerate(param_names) if 'alpha' in elem]\n",
    "    beta_ind = [i for i, elem in enumerate(param_names) if 'beta' in elem]\n",
    "    phi_ind = [i for i, elem in enumerate(param_names) if elem == 'phi']\n",
    "\n",
    "    alpha_ = [i for i, elem in enumerate(param_names) if elem == 'alpha']\n",
    "    beta_ = [i for i, elem in enumerate(param_names) if elem == 'beta']\n",
    "    phi_star_ind = [i for i, elem in enumerate(param_names) if elem == 'phi_star']\n",
    "    \n",
    "    # Reduce df to only columns with parameter values\n",
    "    cols = [f'psi_hat_{company}' for company in df_comp_names['Company']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    persistence = df.iloc[beta_].values + df.iloc[alpha_].values * (1 + df.iloc[phi_ind].values ** 2)\n",
    "    persistence = persistence[0]\n",
    "    df_persistence.loc[len(df_persistence)] = persistence\n",
    "    \n",
    "    if len(alpha_ind) > 1:\n",
    "        tv_persistence = df.iloc[beta_ind].sum().values + df.iloc[alpha_ind].sum().values * (1 + (df.iloc[phi_ind].values + df.iloc[phi_star_ind].values) ** 2)\n",
    "        df_persistence.loc[len(df_persistence)] = tv_persistence[0]\n",
    "        \n",
    "index = [[f'persistence_{model}', f'tv_persistence_0_{model}', f'tv_persistence_1_{model}'] for model in model_names]\n",
    "index = [item for sublist in index for item in sublist]\n",
    "df_persistence.index = index\n",
    "display(df_persistence.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
