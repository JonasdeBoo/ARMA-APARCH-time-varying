{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This template is used to check the robustness of the Sentiment Classification methods. First, 2200 tweets from the Twitter dataset are randomly combined, and must be analysed and classified as negative, neutral or positive. Then, several online datasets with pre-labeled tweets are imported and combined. This combined Dataset will serve as a training set. It is then checked whether the model using the trained set correclty labels the tweet sub-dataset. This will be evaluated against VADER and the extended VADER model, which will be constructed by manually observing patterns in the falsely classified tweets using the VADER method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class loads all relevant packages and dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Load packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all classes necessary for the running of this files. Then import the relevant classes from the Python `thesis_code` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords, wordnet # get stopwords from NLTK library & get all words in english language\n",
    "from nltk.tokenize import word_tokenize # to create word tokens\n",
    "from nltk.stem import WordNetLemmatizer # to reduce words to orginal form\n",
    "from nltk import pos_tag # For Parts of Speech tagging\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Model Building\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Model Metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Vader sentiment classifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load data that returns tweets\n",
    "sys.path.insert(0, os.path.abspath('C:\\\\Users\\\\jdeboo\\\\PycharmProjects\\\\TwitterSentimentGARCH2021\\\\Code\\\\Sentiment analysis and aggregation'))\n",
    "#from sentanalysis import TwitterSentimentAnalysis\n",
    "\n",
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Construct colors for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct self-constructed colormap that will be used throughout this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['seagreen', 'mediumaquamarine', 'steelblue', 'cornflowerblue', 'navy', 'black']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, load the data with company names. Also specify the storage location where the sentiment data must be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of data + file name and location of storage\n",
    "data_loc = r'C:\\Users\\Jonas\\Documents\\Data'\n",
    "\n",
    "# Specify location where daily sentiment scores must be stored\n",
    "store_loc = r'C:\\Users\\Jonas\\Documents\\Data\\Sentiment'\n",
    "\n",
    "# Access company names DataFrame\n",
    "file_name_comp = '\\company_ticker_list_all.xlsx'\n",
    "df_comp_names = pd.read_excel(data_loc + file_name_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `df_train`, by concatenating labeled online DataFrames. Apply the same style to all DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dfs_loc = r'C:\\Users\\Jonas\\OneDrive\\Documenten'\n",
    "\n",
    "# Read and adjust Apple tweets\n",
    "apple_name = 'Apple-Twitter-Sentiment-DFE.csv'\n",
    "df_apple = pd.read_csv(combined_dfs_loc + f'\\\\{apple_name}')\n",
    "rename_dict_apple = {'1' : -1, '3': 0, '5': 1}\n",
    "df_apple = df_apple.replace(to_replace=rename_dict_apple)\n",
    "\n",
    "# Read and adjust airline tweets\n",
    "airline_tweets = 'Tweets.csv'\n",
    "df_airline = pd.read_csv(combined_dfs_loc + f'\\\\{airline_tweets}')\n",
    "rename_dict_airline = {'negative' : -1, 'neutral': 0, 'positive': 1}\n",
    "df_airline = df_airline.replace(to_replace=rename_dict_airline)\n",
    "df_airline['sentiment'] = df_airline.airline_sentiment\n",
    "\n",
    "# processed_tweets\n",
    "neg = 'processedNegative.csv'\n",
    "neu = 'processedNeutral.csv'\n",
    "pos = 'processedPositive.csv'\n",
    "\n",
    "df_neg = pd.read_csv(combined_dfs_loc + f'\\\\{neg}').T.reset_index()\n",
    "df_neg['sentiment'] = -1\n",
    "df_neu = pd.read_csv(combined_dfs_loc + f'\\\\{neu}').T.reset_index()\n",
    "df_neu['sentiment'] = 0\n",
    "df_pos = pd.read_csv(combined_dfs_loc + f'\\\\{pos}').T.reset_index()\n",
    "df_pos['sentiment'] = 1\n",
    "\n",
    "df_twitter = pd.concat([df_neg, df_neu, df_pos]).reset_index(drop=True)\n",
    "df_twitter['text'] = df_twitter['index']\n",
    "\n",
    "# Get Sentiment140 dataset\n",
    "df_senti140 = pd.read_csv(combined_dfs_loc + f'\\\\sentiment140.csv', header=None)\n",
    "rename_dict_senti140 = {0 : -1, 4: 1}\n",
    "df_senti140 = df_senti140.replace(to_replace=rename_dict_senti140).sample(8000)\n",
    "df_senti140[['sentiment', 'text']] = df_senti140[[0, 5]]\n",
    "\n",
    "# Merge dfs\n",
    "cols = ['text', 'sentiment']\n",
    "df_train = pd.concat([df_apple[cols], df_twitter[cols], df_senti140[cols], df_airline[cols]]).reset_index(drop=True)\n",
    "df_train.to_csv(store_loc + f'\\\\df_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `df_test`, by extracting 2200 random tweets from all the Twitter datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(store_loc + f'\\\\df_test.csv')\n",
    "df_test = df_test[df_test.sentiment.isin(['-1', '0', '1'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Create sentiment classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section several supervised classification algorithms are used in order to compare the performance based on the humanly annotated `df_test`, which is a sample of the tweets in our dataset on which we like to perform sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Supervised classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the supervised approach is used to classify the tweets. This is done using the Multinomial Naive Bayes Classifier.\n",
    "\n",
    "Rebalance the training set and determine helper function used for preprocessing or Part-of-Speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebalance training dataset\n",
    "df_train = df_train[df_train.sentiment != 'not_relevant']\n",
    "smallest_sample = min(df_train['sentiment'].value_counts())\n",
    "list_train = [df_train[df_train['sentiment'] == sent].sample(smallest_sample) for sent in df_train['sentiment'].unique()]\n",
    "df_train = pd.DataFrame()\n",
    "df_train = df_train.append([item for item in list_train])\n",
    "\n",
    "def labelling(Rows):\n",
    "    if(Rows['sentiment'] > 0):\n",
    "        Label = 'positive'\n",
    "    elif(Rows['sentiment'] < 0):\n",
    "        Label = 'negative'\n",
    "    else:\n",
    "        Label = 'neutral'\n",
    "    return Label\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to remove punctuations, links, emojis, and stop words\n",
    "def preprocessTweets(tweet):\n",
    "    tweet = tweet.lower()  #has to be in place\n",
    "    text = []\n",
    "    Lemmatizer = WordNetLemmatizer().lemmatize\n",
    "\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#|\\d+', '', tweet)\n",
    "\n",
    "    # Remove possible links\n",
    "    tweet = re.sub('https?://\\S+|www\\.\\S+', '', tweet)\n",
    "    tweet = ''.join([t for t in tweet if t not in string.punctuation])   # remove all punctuations       \n",
    "    tweet = ''.join([t for t in tweet if not t.isdigit()])   # remove all numeric digits   \n",
    "    tweet = re.sub(\"[^a-zA-Z0-9]\", \" \", tweet)   # letters only \n",
    "    \n",
    "    # Tokenize string\n",
    "    tweet_tokens = word_tokenize(str(tweet))\n",
    "\n",
    "    # Defining my NLTK stop words\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    word_list = ['aapl', 'apple', 'flight', 'airline', 'airtravel', 'airpassenger', 'delayed', 'gate', 'terminal']\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tweet_tokens = [w for w in tweet_tokens if w not in stop_words]\n",
    "    tweet_tokens = [w for w in tweet_tokens if w not in word_list]\n",
    "\n",
    "    # Assign Part-of-Speech to all words that are not stopwords\n",
    "    word_pos = pos_tag(tweet_tokens)\n",
    "        \n",
    "    # Lemmatize all words\n",
    "    lemm_words = [Lemmatizer(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "\n",
    "    return(\" \".join(lemm_words))\n",
    "\n",
    "def important_features(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    topn_negative = sorted(zip(classifier.feature_count_[0], feature_names), reverse = True)[:n]\n",
    "    topn_neutral = sorted(zip(classifier.feature_count_[1], feature_names), reverse = True)[:n]\n",
    "    topn_positive = sorted(zip(classifier.feature_count_[2], feature_names), reverse = True)[:n]\n",
    "\n",
    "    #print(classifier.feature_count_)\n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important features in negative tweets\")\n",
    "\n",
    "    for coef, feat in topn_negative:\n",
    "        print(class_labels[-1], coef, feat)\n",
    "\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important features in neutral tweets\")\n",
    "\n",
    "    for coef, feat in topn_neutral:\n",
    "        print(class_labels[0], coef, feat)\n",
    "        \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important features in positive tweets\")\n",
    "\n",
    "    for coef, feat in topn_positive:\n",
    "        print(class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess training data, and encode target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess target and explanatory variales\n",
    "df_train['sentiment'] = df_train.apply(labelling, axis = 1)\n",
    "df_train['text'] = df_train['text'].apply(lambda tweet: preprocessTweets(tweet))\n",
    "\n",
    "# Encode target variable\n",
    "Encoder = LabelEncoder()\n",
    "df_train[\"sentiment\"] = Encoder.fit_transform(df_train[\"sentiment\"])\n",
    "\n",
    "# Defining our vectorizer with total words of 50000 and with uni to trigram model\n",
    "TF_IDF = TfidfVectorizer(max_features = 20000, ngram_range = (1, 2))\n",
    "\n",
    "# Fitting and transforming our reviews into a matrix of weighed words\n",
    "X = TF_IDF.fit_transform(df_train['text'])\n",
    "\n",
    "# Declaring our target variable\n",
    "y = df_train[\"sentiment\"]\n",
    "\n",
    "# Instantiate model and fit \n",
    "Bayes = MultinomialNB(class_prior=[1/3, 1/3, 1/3])\n",
    "model = Bayes.fit(X, y)\n",
    "\n",
    "# Print the 20 most important features.\n",
    "important_features(TF_IDF, model, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on subsample of actual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode df_test labels.\n",
    "df_test = pd.read_csv(store_loc + f'\\\\df_test.csv').fillna(1000)\n",
    "df_test = df_test[df_test.sentiment != 1000]\n",
    "df_test.sentiment[df_test.sentiment == '-'] = -1\n",
    "df_test['sentiment'] = df_test.sentiment.astype(float)\n",
    "df_test['sentiment'] = df_test.apply(labelling, axis=1)\n",
    "\n",
    "# Get y_test and x_test\n",
    "y_test = Encoder.transform(df_test.sentiment)\n",
    "X_test = df_test.text\n",
    "\n",
    "# Preprocess tweets using the preprocessTweets functionality and transform features using tf-idf vectorizer\n",
    "X_test = X_test.apply(lambda tweet: preprocessTweets(tweet))\n",
    "X_test = TF_IDF.transform(X_test)\n",
    "\n",
    "# Predict target variables \n",
    "y_pred = model.predict(X_test)\n",
    "df_test['Naive Bayes'] = Encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Create confusion matrix\n",
    "ConfusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plotting Function for Confusion Matrix\n",
    "def plot_cm(cm, classes, title, normalized = False, cmap = plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    plt.title(title, pad = 20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalized:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[: np.newaxis]\n",
    "        cm = np.round(cm, 3)\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print(\"Unnormalized Confusion Matrix\")\n",
    "\n",
    "    threshold = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment = \"center\", color = \"white\" if cm[i, j] > threshold else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Predicted Label\", labelpad = 20)\n",
    "plt.ylabel(\"Real Label\", labelpad = 20)\n",
    "\n",
    "plot_cm(ConfusionMatrix, classes = [\"Positive\", \"Neutral\", \"Negative\"], title = \"Confusion Matrix of Sentiment Analysis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Untrained VADER sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using VADER sentiment.\n",
    "\n",
    "First, define preprocessingsteps voor VADER sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessVader(tweet):   \n",
    "    # Remove urls\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#|\\d+', '', tweet)\n",
    "    \n",
    "    # Remove possible links\n",
    "    tweet = re.sub('https?://\\S+|www\\.\\S+', '', tweet)\n",
    "\n",
    "    return(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data to perform sentiment classification on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "compounded_sentiment_series = []\n",
    "\n",
    "for tweet in df_test.text:\n",
    "    tweet = preprocessVader(tweet)\n",
    "    \n",
    "    # Perform sentiment analysis on seperate sentences within each tweet\n",
    "    tokenized_tweets = sent_tokenize(tweet)\n",
    "\n",
    "    compound_sentiment_tokenized_tweets = []\n",
    "    for sentence in tokenized_tweets:\n",
    "        compound_sentiment_tokenized_tweets += [analyzer.polarity_scores(sentence)['compound']]\n",
    "\n",
    "    compounded_sentiment_series.append(np.mean(compound_sentiment_tokenized_tweets))\n",
    "\n",
    "# Calculate the direction of the sentiment, if compounded sentiment > 0.05, direction is positive, if compounded\n",
    "# sentiment < -.05 the direction is negative, otherwise the direction is neutral\n",
    "senti_direction_series = []\n",
    "for sentiment in compounded_sentiment_series:\n",
    "    if sentiment > 0.05:\n",
    "        senti_direction_series.append('positive')\n",
    "    elif sentiment < -0.05:\n",
    "        senti_direction_series.append('negative')\n",
    "    else:\n",
    "        senti_direction_series.append('neutral')\n",
    "        \n",
    "df_test['VADER'] = senti_direction_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Trained VADER classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train VADER to take into account domain of `df_test`. This is done by looking at misspecified tweets by simply using VADER. An feature importance analyzer is made to determine the most importance features in positive and negative tweets. These features are then added to the lexicon, if they are deemed appropriately functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataFrame with misspecified entries\n",
    "df_wrong = df_test[df_test.sentiment != df_test.VADER]\n",
    "df_wrong['text'] = df_wrong['text'].apply(lambda tweet: preprocessTweets(tweet))\n",
    "\n",
    "# Defining our vectorizer with total words of 50000 and with uni to trigram model\n",
    "TF_IDF = TfidfVectorizer(max_features = 1000, ngram_range = (1, 2))\n",
    "\n",
    "# Fitting and transforming our reviews into a matrix of weighed words\n",
    "X = TF_IDF.fit_transform(df_wrong['text'])\n",
    "\n",
    "# Declaring our target variable\n",
    "y = df_wrong[\"sentiment\"]\n",
    "\n",
    "# Instantiate model and fit \n",
    "Bayes = MultinomialNB()\n",
    "model = Bayes.fit(X, y)\n",
    "\n",
    "# Print the 20 most important features.\n",
    "important_features(TF_IDF, model, n=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our intuition and the results presented above, try to extend the VADER lexicon to the specific domain of the test tweets, without 'overfitting' to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of new words\n",
    "new_words = {'sustainable': 1.5,\n",
    "             'innovation': 1.5,\n",
    "             'global warming': -1.5,\n",
    "             'pollution': -1.5,\n",
    "             'human': 1.5,\n",
    "             'responsible': 1.5,\n",
    "             'restore': 1.5,\n",
    "             'environmental damage': -1.5,\n",
    "             'layoff': -1.5,\n",
    "             'un-green': -1.5,\n",
    "             'ecofriendly': 1.5,\n",
    "             'ESG': 1.5,\n",
    "             'lawsuit': -1.5,\n",
    "             'sued': -1.5,\n",
    "             'allegation': -1.5,\n",
    "             'discrimination': -1.5,\n",
    "             'environmental': 1.5,\n",
    "             'unequality': -1.5,\n",
    "             'unequal': -1.5,\n",
    "             'greenhouse gas': -1.5,\n",
    "             'emmission': -1.5,\n",
    "             'oil': -1.5,\n",
    "             'favoritism': -1.5,\n",
    "             'disrespectful': -1.5,\n",
    "             'asocial': -1.5,\n",
    "             'social': 1.5, \n",
    "             'trash': -1.5,\n",
    "             'wasteful':-1.5,\n",
    "             'garbage': -1.5,\n",
    "             'plastic waste': -1.5, \n",
    "             'lose': -1.5,\n",
    "             'abuse': -1.5,\n",
    "             'impoverished': -1.5,\n",
    "             'toxic': -1.5,\n",
    "             'dumping': -1.5,\n",
    "             'obesity': -1.5,\n",
    "             'clean': 1.5,\n",
    "             'cleanenergy': 1.5,\n",
    "             'renewable': 1.5,\n",
    "             'impact investing': 1.5,\n",
    "             'CSR': 1.5,\n",
    "             'human rights': 1.5,\n",
    "             'recycling': 1.5,\n",
    "             'renewables': 1.5,\n",
    "             'green': 1.5,\n",
    "             'ungreen': -1.5,\n",
    "             'environmentally friendly': 1.5,\n",
    "             'sexism': -1.5,\n",
    "             'ethical': 1.5,\n",
    "             'climate change': -1.5,\n",
    "             'climatechange':-1.5,\n",
    "             'climate disaster': -1.5,\n",
    "             'climate disruption': -1.5,\n",
    "             'polluting': -1.5,\n",
    "             'harassment': -1.5,\n",
    "             'unsafe': -1.5,\n",
    "             'insecure': -1.5,\n",
    "             \n",
    "}\n",
    "\n",
    "# Instantiate Sentiment Intensity Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Update lexicon\n",
    "analyzer.lexicon.update(new_words)\n",
    "\n",
    "compounded_sentiment_series = []\n",
    "\n",
    "for tweet in df_test.text:\n",
    "    tweet = preprocessVader(tweet)\n",
    "    \n",
    "    # Perform sentiment analysis on seperate sentences within each tweet\n",
    "    tokenized_tweets = sent_tokenize(tweet)\n",
    "\n",
    "    compound_sentiment_tokenized_tweets = []\n",
    "    for sentence in tokenized_tweets:\n",
    "        compound_sentiment_tokenized_tweets += [analyzer.polarity_scores(sentence)['compound']]\n",
    "\n",
    "    compounded_sentiment_series.append(np.mean(compound_sentiment_tokenized_tweets))\n",
    "\n",
    "# Calculate the direction of the sentiment, if compounded sentiment > 0.05, direction is positive, if compounded\n",
    "# sentiment < -.05 the direction is negative, otherwise the direction is neutral\n",
    "senti_direction_series = []\n",
    "for sentiment in compounded_sentiment_series:\n",
    "    if sentiment > 0.05:\n",
    "        senti_direction_series.append('positive')\n",
    "    elif sentiment < -0.05:\n",
    "        senti_direction_series.append('negative')\n",
    "    else:\n",
    "        senti_direction_series.append('neutral')\n",
    "        \n",
    "df_test['adjusted VADER'] = senti_direction_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct F1 and Accuracy. Additionally, calculate the Confusion matrix and check balancedness of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how balanced the sample is\n",
    "n_pos = len(df_test[df_test.sentiment == 'positive'])\n",
    "n_neg = len(df_test[df_test.sentiment == 'negative'])\n",
    "n_neu = len(df_test[df_test.sentiment == 'neutral'])\n",
    "n = len(df_test)\n",
    "\n",
    "print(n_pos / n , n_neg / n, n_neu / n)\n",
    "\n",
    "# Create confusion matrix to check occurency of falsely specified (polarity shifted) instance\n",
    "ConfusionMatrix = confusion_matrix(df_test.sentiment, df_test['adjusted VADER'])\n",
    "plot_cm(ConfusionMatrix, classes = [\"Positive\", \"Neutral\", \"Negative\"], title = \"Confusion Matrix of Sentiment Analysis\")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(figsize = (20,4), nrows = 1, ncols = 3)\n",
    "cols = ['Naive Bayes', 'VADER', 'adjusted VADER']\n",
    "\n",
    "for col, j in zip(cols, range(len(axs))):    \n",
    "    f1 = f1_score(df_test['sentiment'], df_test[col], average='macro')\n",
    "    accuracy = accuracy_score(df_test['sentiment'], df_test[col])\n",
    "    print(f1, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
